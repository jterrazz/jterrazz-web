![](assets/thumbnail.jpg)

# Ma√Ætriser la gestion de la m√©moire, le jour o√π j'ai cod√© mon `malloc`

Vous √™tes-vous d√©j√† demand√© comment votre ordinateur jongle avec des milliards d'octets chaque seconde? ü§π‚Äç‚ôÇÔ∏è C'est une question qui m'a toujours fascin√©. J'ai donc d√©cid√© de soulever le capot pour explorer l'une des pi√®ces ma√Ætresses de cette m√©canique: **l'allocation dynamique de la m√©moire**.

Dans cet article, je vous guide √† travers la raison d'√™tre de `malloc`, ses rouages internes, et le r√©cit de la cr√©ation de ma propre version, de A √† Z, en m'appuyant sur l'appel syst√®me `mmap`. Si l'id√©e vous intimide, n'ayez crainte: je vais tout d√©composer, en repartant des fondations. Pour moi, percer ces myst√®res fut une r√©v√©lation. Et pour ceux qui aiment mettre les mains dans le cambouis, [mon projet final est disponible sur GitHub](https://github.com/jterrazz/42-malloc). Embarquons. üöÄ

```c
// Voici les fonctions que nous allons construire.
void  malloc(size_t size);
void  free(void* ptr);
void  realloc(void* ptr, size_t size);
void  calloc(size_t count, size_t size);

// C'est ainsi que nous demanderons de la m√©moire au syst√®me d'exploitation.
#include <sys/mman.h>

void* mmap(void* addr, size_t len, int prot, int flags, int fd, off_t offset);
int   munmap(void* addr, size_t len);

// Et ceci nous aidera √† d√©finir quelques r√®gles de base.
#include <sys/resource.h>

int   getrlimit(int resource, struct rlimit* rlp);
int   setrlimit(int resource, const struct rlimit* rlp);
```

## Les trois visages de la m√©moire: statique, sur la pile et dynamique

Revenons un instant aux fondamentaux: comment le C g√®re-t-il la m√©moire par d√©faut? C'est un syst√®me plut√¥t rigide.

- **Variables statiques et globales**: Elles sont fig√©es dans le marbre √† la compilation. Leur place est r√©serv√©e pour toute la dur√©e de vie du programme, au m√™me titre que le code ex√©cutable lui-m√™me.
- **Variables automatiques**: Celles que l'on d√©clare au sein des fonctions. Elles naissent sur la " stack " (la pile) √† l'appel d'une fonction et s'√©vanouissent d√®s que celle-ci se termine.

Ce syst√®me fonctionne, mais il bute sur deux limitations majeures:

1. **La taille de toutes les donn√©es doit √™tre connue √† l'avance.** Impossible, donc, de cr√©er un tableau dont la taille serait d√©cid√©e en cours de route.
2. **La dur√©e de vie est immuable.** La m√©moire persiste soit pour toute l'ex√©cution du programme, soit le temps d'un appel de fonction. Pas d'entre-deux.

C'est ici qu'entre en sc√®ne l'allocation dynamique, con√ßue pour toutes les situations o√π l'on ne conna√Æt ni le " quoi " ni le " quand " au moment de la compilation.

### `mmap`: le dialogue direct avec le noyau

```c
#include <sys/mman.h>

void* mmap(void* addr, size_t len, int prot, int flags, int fd, off_t offset);
```

Comment donc obtenir de la m√©moire √† la demande? Il faut la solliciter aupr√®s du syst√®me d'exploitation. Le noyau met √† notre disposition un outil puissant pour cela: un **appel syst√®me** (*system call*). Celui sur lequel je me suis concentr√© est `mmap()`. Imaginez une ligne directe avec le noyau, lui demandant de r√©server une parcelle de m√©moire physique et de la " mapper " √† une adresse virtuelle dans notre programme. C'est notre source premi√®re de m√©moire brute. üåå

Il existe un autre outil, `sbrk`, mais pour ce projet, `mmap` est notre arme de pr√©dilection. Sa souplesse pour manipuler des r√©gions m√©moire est sans √©gale.

### Si `mmap` est la source, pourquoi s'encombrer de `malloc`?

Ce fut ma premi√®re grande interrogation. Si `mmap` nous fournit la m√©moire, pourquoi ne pas simplement l'appeler chaque fois que nous en avons besoin?

La r√©ponse tient en un mot: performance. Les appels syst√®me sont co√ªteux. Chaque appel implique un changement de contexte (*context switch*) entre votre programme et le noyau, une op√©ration particuli√®rement co√ªteuse en temps. La plupart des applications demandent et lib√®rent de petites quantit√©s de m√©moire des milliers de fois par seconde. Si chacune de ces requ√™tes √©tait un appel syst√®me √† part enti√®re, les performances de nos applications s'effondreraient.

C'est l√† que `malloc` entre en jeu. Il joue le r√¥le d'un gestionnaire avis√©. Au lieu que vous ne sollicitiez le noyau pour chaque broutille, `malloc` demande au syst√®me *une seule fois* un vaste territoire de m√©moire, qu'il se charge ensuite de morceler pour vous. Lorsque vous demandez un peu de m√©moire, `malloc` se contente de d√©couper une tranche du segment qu'il d√©tient d√©j√†. Certes, cela ajoute une l√©g√®re surcouche de gestion (*overhead*), mais le gain en vitesse est colossal. C'est un compromis d'ing√©nierie des plus classiques.

## Passons √† la construction: mon impl√©mentation

### La biblioth√®que: la bo√Æte √† outils m√©moire

Ma biblioth√®que `malloc` fournit le trio classique:

- `malloc`: Demande un bloc de m√©moire et retourne un pointeur vers celui-ci.
- `free`: R√©cup√®re ce pointeur lorsque vous avez termin√© et marque la m√©moire comme de nouveau disponible.
- `realloc`: Permet de redimensionner un bloc de m√©moire d√©j√† allou√©, tout en pr√©servant les donn√©es qu'il contient.

### La structure des donn√©es: l'anatomie de ma m√©moire

Pour que cela fonctionne, je devais savoir √† tout instant o√π se trouvait chaque allocation. Mon approche repose sur une hi√©rarchie √† deux niveaux:

- **Heap** (ou tas): Une large r√©gion de m√©moire que je demande au syst√®me d'exploitation via `mmap`.
- **Bloc**: Un plus petit morceau d'un *heap* que je distribue √† chaque appel √† `malloc`.

Ces deux entit√©s n√©cessitent des m√©tadonn√©es. J'ai donc plac√© un petit en-t√™te (*header*) juste avant chaque *heap* et chaque *bloc* pour y stocker des informations cruciales. Apr√®s un simple appel √† `malloc`, la carte m√©moire ressemble √† ceci:

![Heap and Block Structure](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iXHfrEUza03cFe5IXEvs0Q.png)

Voici les `structs` C que j'ai d√©finies pour ces m√©tadonn√©es:

```c
// M√©tadonn√©es pour une r√©gion enti√®re mapp√©e par mmap
typedef struct s_heap {
  struct s_heap   *prev;
  struct s_heap   *next;
  t_heap_group    group; // TINY, SMALL, ou LARGE
  size_t          total_size;
  size_t          free_size;
  size_t          block_count;
} t_heap;

// M√©tadonn√©es pour un seul bloc allou√©
typedef struct s_block {
  struct s_block  *prev;
  struct s_block  *next;
  size_t          data_size;
  bool            freed;
} t_block;
```

En dotant chaque bloc de pointeurs `next` et `prev`, j'ai de fait cr√©√© une liste doublement cha√Æn√©e. Cette cha√Æne me permet de traverser le *heap* pour d√©nicher un espace libre ou pour identifier les voisins d'un bloc que je souhaite lib√©rer.

Ces petites macros m'ont servi de raccourcis pour naviguer rapidement du d√©but d'un *heap* ou d'un *bloc* √† la zone de donn√©es utilisable par l'utilisateur.

```c
#define HEAP_SHIFT(start)   ((void*)start + sizeof(t_heap))
#define BLOCK_SHIFT(start)  ((void*)start + sizeof(t_block))
```

### Strat√©gie de performance: toutes les allocations ne naissent pas √©gales

J'ai vite compris que traiter une allocation de 10 octets de la m√™me mani√®re qu'une de 10 m√©gaoctets relevait du non-sens. Pour optimiser, j'ai cr√©√© trois cat√©gories: `TINY`, `SMALL` et `LARGE`. Ma strat√©gie: pr√©-allouer des pages m√©moire pour les requ√™tes `TINY` et `SMALL`, avec pour objectif de pouvoir y loger au moins 100 blocs. Les allocations `LARGE` sont une exception; elles sont g√©r√©es au cas par cas, sans pr√©-allocation, car elles sont bien plus rares.

Une astuce d'initi√©, apprise sur le tas: il est bien plus efficace de choisir des tailles de *heap* qui soient des multiples de la taille de page syst√®me. On peut l'obtenir avec `getpagesize()` (ou `getconf PAGE_SIZE` dans le terminal). Sur ma machine, c'est 4096 octets.

J'ai donc fait quelques calculs pour d√©finir les tailles de mes *heaps*:

```c
// Une page peut contenir 128 blocs "tiny"
#define  TINY_HEAP_ALLOCATION_SIZE   (4 * getpagesize())
#define  TINY_BLOCK_SIZE             (TINY_HEAP_ALLOCATION_SIZE / 128)

// Quatre pages peuvent contenir 128 blocs "small"
#define  SMALL_HEAP_ALLOCATION_SIZE  (16 * getpagesize())
#define  SMALL_BLOCK_SIZE            (SMALL_HEAP_ALLOCATION_SIZE / 128)
```

### L'algorithme de `malloc`: trouver un foyer pour les donn√©es

Lorsqu'un appel √† `malloc` survient, voici la logique que suit mon code:

1. Il consulte d'abord un pointeur global pour voir si des *heaps* existent d√©j√†.
2. Il parcourt ensuite la liste des *heaps*, √† la recherche d'un bloc libre suffisamment grand. J'ai opt√© pour une strat√©gie **first-fit** (*premier ajustement*): prendre le premier qui convient. C'est simple et g√©n√©ralement rapide.
3. S'il arrive √† la fin d'un *heap* o√π il reste de l'espace, il y greffe simplement un nouveau bloc.
4. Si tous les *heaps* existants sont pleins, il est temps de demander plus de " terrain " au syst√®me d'exploitation en appelant `mmap`.

```c
// L'appel syst√®me pour cr√©er un nouveau heap.
void *heap = (t_heap *)mmap(NULL, heap_size, PROT_READ | PROT_WRITE, MAP_PRIVATE | MAP_ANON, -1, 0);
```

### `free` et le spectre de la fragmentation

![Memory Fragmentation](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y7xikxHO1Yoyv1eZm7l6aA.png)

Quand `free` est appel√©, marquer un bloc comme " disponible " est simple. Mais cela cr√©e un probl√®me redoutable: la **fragmentation**. On se retrouve alors avec une m√©moire mit√©e de petits trous inutilisables, tel un gruy√®re ou une partie de Tetris qui aurait vir√© au cauchemar.

Pour contrer ce ph√©nom√®ne, j'ai mis en ≈ìuvre deux strat√©gies cl√©s:

- **La fusion (*coalescing*)**: Lorsqu'un bloc est lib√©r√©, je v√©rifie si ses voisins imm√©diats sont eux aussi libres. Si oui, je les fusionne pour former un seul grand bloc disponible.
- **La restitution de la m√©moire**: Si le bloc en cours de lib√©ration est le tout dernier d'un *heap* et que d'autres *heaps* sont encore actifs, je lib√®re l'int√©gralit√© du *heap* devenu vide en le retournant au syst√®me d'exploitation via `munmap`. Inutile de conserver de la m√©moire inoccup√©e.

```c
// Rendre la m√©moire au noyau.
munmap(heap, heap->total_size);
```

### `realloc`: le polymorphe

`realloc` n'est, au fond, qu'une chor√©graphie bien huil√©e: allouer un nouveau bloc plus grand avec `malloc`, copier les donn√©es de l'ancien bloc vers le nouveau avec `memcpy`, puis lib√©rer l'ancien avec `free`.

Il y a un cas limite √† conna√Ætre: `realloc(ptr, 0)`. Le comportement attendu peut varier. J'ai adopt√© une approche " paresseuse " en retournant simplement le pointeur original. Cependant, certaines normes stipulent que cela devrait √™tre √©quivalent √† `free(ptr)`. Mon conseil: n'utilisez pas `realloc` pour lib√©rer de la m√©moire. √Ä chaque outil sa fonction.

## L'√©preuve du feu

La partie la plus gratifiante fut de voir mon `malloc` faire tourner de vrais programmes. J'ai √©crit un petit script pour forcer l'√©diteur de liens dynamique (*dynamic linker*) √† charger ma biblioth√®que √† la place de celle du syst√®me.

```sh
#!/bin/sh
export DYLD_LIBRARY_PATH=.
export DYLD_INSERT_LIBRARIES=libft_malloc.so
export DYLD_FORCE_FLAT_NAMESPACE=1
$@
```

En sauvegardant ce fichier sous le nom `run.sh`, je pouvais lancer des commandes comme `sh run.sh ls -l` ou `sh run.sh vim` et observer le comportement.

### Le crash de `vim` et la le√ßon sur l'alignement

√âvidemment, le succ√®s ne fut pas imm√©diat. Si `ls` se lan√ßait sans broncher, `vim`, lui, provoquait une erreur de segmentation fatale. Que se passait-il?

Le coupable: **l'alignement m√©moire** (*memory alignment*). J'ai d√©couvert que le `malloc` standard de macOS (o√π je faisais mes tests) ne se contente pas de retourner un pointeur. Il garantit que l'adresse fournie est un multiple de 16. Certains programmes et certaines instructions en d√©pendent pour des raisons de performance. Mon `malloc` ne respectait pas cette contrainte, et `vim` plantait.

La solution tenait en une ligne, une astuce binaire (*bitwise*) aussi simple qu'efficace: `size = (size + 15) & ~15;`. Cette unique ligne de code assure que la taille allou√©e est syst√©matiquement un multiple de 16, et que l'adresse retourn√©e sera donc correctement align√©e. Une le√ßon capitale, apprise √† la dure.

Ainsi s'ach√®ve notre p√©riple. Partis d'un simple appel syst√®me au noyau, `mmap`, nous avons abouti √† une biblioth√®que `malloc` compl√®te et √©prouv√©e. Pour moi, ce projet n'√©tait pas seulement une question de code; le v√©ritable enjeu √©tait de d√©mystifier une m√©canique au c≈ìur de nos machines.

Rien ne vaut la pratique. Si vous souhaitez creuser le sujet, n'h√©sitez pas √† consulter [l'impl√©mentation compl√®te sur mon GitHub](https://github.com/jterrazz/42-malloc). Forkez-le, cassez-le, am√©liorez-le. Car comprendre les fondations, c'est se donner le pouvoir de tout construire. Bon code √† tous.
